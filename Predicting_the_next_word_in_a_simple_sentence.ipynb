{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEL66OdbtOGJ",
        "outputId": "cab86736-b97d-40c2-bf13-22913bbdf334"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "\n",
        "class Tokenizer:\n",
        "    def __init__(self, data_path):\n",
        "        with open(data_path, 'r', encoding='utf8') as f:\n",
        "            self.data = f.read().replace('\\n', '')\n",
        "\n",
        "    def tokenize(self):\n",
        "        return list(self.data)\n",
        "\n",
        "class Vocab:\n",
        "    def __init__(self, tokens):\n",
        "        self.token_to_index = {}\n",
        "        self.index_to_token = {}\n",
        "        self.token_frequency = {}\n",
        "\n",
        "        self.add_token('<PAD>')\n",
        "        self.add_token('<UNK>')\n",
        "\n",
        "        for token in tokens:\n",
        "            self.add_token(token)\n",
        "\n",
        "    def add_token(self, token):\n",
        "        if token not in self.token_to_index:\n",
        "            index = len(self.token_to_index)\n",
        "            self.token_to_index[token] = index\n",
        "            self.index_to_token[index] = token\n",
        "            self.token_frequency[token] = 1\n",
        "        else:\n",
        "            self.token_frequency[token] += 1\n",
        "\n",
        "    def lookup_token(self, token):\n",
        "        if token in self.token_to_index:\n",
        "            return self.token_to_index[token]\n",
        "        else:\n",
        "            return self.token_to_index['<UNK>']\n",
        "\n",
        "    def lookup_index(self, index):\n",
        "        if index in self.index_to_token:\n",
        "            return self.index_to_token[index]\n",
        "        else:\n",
        "            return '<UNK>'\n",
        "\n",
        "class SimpleLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
        "        super(SimpleLanguageModel, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
        "        self.rnn = nn.GRU(embedding_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, input, hidden=None):\n",
        "        embedded = self.embedding(input)\n",
        "        output, hidden = self.rnn(embedded, hidden)\n",
        "        output = self.fc(output)\n",
        "        return output, hidden\n",
        "\n",
        "def train_model(model, train_data, num_epochs, learning_rate):\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for input, target in train_data:\n",
        "            model.zero_grad()\n",
        "\n",
        "            output, hidden = model(input)\n",
        "            loss = criterion(output.view(-1, model.vocab_size), target.view(-1))\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print('Epoch:', epoch + 1, 'Loss:', total_loss)\n",
        "\n",
        "    return model\n",
        "\n",
        "def predict(model, input_text, vocab, tokenizer):\n",
        "    #input_tokens = tokenizer.tokenize(input_text)\n",
        "    input_tokens = word_tokenize(input_text)\n",
        "\n",
        "    input_token_ids = [vocab.lookup_token(token) for token in input_tokens]\n",
        "    input_tensor = torch.LongTensor([input_token_ids])\n",
        "\n",
        "    hidden = None\n",
        "    output_tokens = []\n",
        "    used_tokens = {}\n",
        "    with torch.no_grad():\n",
        "        while True:\n",
        "            output, hidden = model(input_tensor, hidden)\n",
        "            last_output = output.squeeze(0)[-1]\n",
        "            probabilities = nn.functional.softmax(last_output, dim=0)\n",
        "            next_token_id = torch.argmax(probabilities).item()\n",
        "            output_tokens.append(next_token_id)\n",
        "            if vocab.lookup_index(next_token_id) == '<EOS>' or len(output_tokens) >= 1000:\n",
        "                break\n",
        "            input_tensor = torch.LongTensor([[next_token_id]])\n",
        "        \n",
        "\n",
        "    output_text = [vocab.lookup_index(token_id) for token_id in output_tokens]\n",
        "    output_text = ' '.join(output_text)\n",
        "    return output_text"
      ],
      "metadata": {
        "id": "2_Qs2wA_6DKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# متن آموزشی را می‌خوانیم\n",
        "tokenizer = Tokenizer('/content/data.txt')\n",
        "tokens = tokenizer.tokenize()\n",
        "vocab = Vocab(tokens)\n",
        "# داده‌ها را تبدیل به بردارهای عددی می‌کنیم\n",
        "token_ids = [vocab.lookup_token(token) for token in tokens]\n",
        "input_sequence = torch.LongTensor(token_ids[:-1]).unsqueeze(0)\n",
        "target_sequence = torch.LongTensor(token_ids[1:]).unsqueeze(0)\n",
        "# تعریف مدل و آموزش آن\n",
        "model = SimpleLanguageModel(len(vocab.token_to_index), 50, 100)\n",
        "model = train_model(model, [(input_sequence, target_sequence)], 500, 0.001)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rHay016r-BH",
        "outputId": "d1bcdc88-9e1b-457f-95d7-7e6e7baf8dbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 55 Loss: 1.3982572555541992\n",
            "Epoch: 56 Loss: 1.3685566186904907\n",
            "Epoch: 57 Loss: 1.3390238285064697\n",
            "Epoch: 58 Loss: 1.3096495866775513\n",
            "Epoch: 59 Loss: 1.2804265022277832\n",
            "Epoch: 60 Loss: 1.2513537406921387\n",
            "Epoch: 61 Loss: 1.2224376201629639\n",
            "Epoch: 62 Loss: 1.193691372871399\n",
            "Epoch: 63 Loss: 1.1651310920715332\n",
            "Epoch: 64 Loss: 1.1367707252502441\n",
            "Epoch: 65 Loss: 1.1086207628250122\n",
            "Epoch: 66 Loss: 1.0806893110275269\n",
            "Epoch: 67 Loss: 1.052986979484558\n",
            "Epoch: 68 Loss: 1.0255272388458252\n",
            "Epoch: 69 Loss: 0.9983252286911011\n",
            "Epoch: 70 Loss: 0.9713958501815796\n",
            "Epoch: 71 Loss: 0.9447532892227173\n",
            "Epoch: 72 Loss: 0.9184126853942871\n",
            "Epoch: 73 Loss: 0.8923919200897217\n",
            "Epoch: 74 Loss: 0.8667120933532715\n",
            "Epoch: 75 Loss: 0.8413950800895691\n",
            "Epoch: 76 Loss: 0.8164600133895874\n",
            "Epoch: 77 Loss: 0.791924238204956\n",
            "Epoch: 78 Loss: 0.767803430557251\n",
            "Epoch: 79 Loss: 0.744114875793457\n",
            "Epoch: 80 Loss: 0.7208753824234009\n",
            "Epoch: 81 Loss: 0.6981005668640137\n",
            "Epoch: 82 Loss: 0.6758043169975281\n",
            "Epoch: 83 Loss: 0.6539982557296753\n",
            "Epoch: 84 Loss: 0.6326929330825806\n",
            "Epoch: 85 Loss: 0.6118972897529602\n",
            "Epoch: 86 Loss: 0.5916186571121216\n",
            "Epoch: 87 Loss: 0.5718628764152527\n",
            "Epoch: 88 Loss: 0.5526336431503296\n",
            "Epoch: 89 Loss: 0.5339332818984985\n",
            "Epoch: 90 Loss: 0.515762209892273\n",
            "Epoch: 91 Loss: 0.4981195032596588\n",
            "Epoch: 92 Loss: 0.4810030162334442\n",
            "Epoch: 93 Loss: 0.4644097089767456\n",
            "Epoch: 94 Loss: 0.448335736989975\n",
            "Epoch: 95 Loss: 0.43277642130851746\n",
            "Epoch: 96 Loss: 0.41772547364234924\n",
            "Epoch: 97 Loss: 0.40317487716674805\n",
            "Epoch: 98 Loss: 0.38911598920822144\n",
            "Epoch: 99 Loss: 0.3755395710468292\n",
            "Epoch: 100 Loss: 0.3624361753463745\n",
            "Epoch: 101 Loss: 0.349796324968338\n",
            "Epoch: 102 Loss: 0.3376102149486542\n",
            "Epoch: 103 Loss: 0.3258681893348694\n",
            "Epoch: 104 Loss: 0.31456002593040466\n",
            "Epoch: 105 Loss: 0.3036758601665497\n",
            "Epoch: 106 Loss: 0.2932051718235016\n",
            "Epoch: 107 Loss: 0.2831379771232605\n",
            "Epoch: 108 Loss: 0.27346405386924744\n",
            "Epoch: 109 Loss: 0.2641732096672058\n",
            "Epoch: 110 Loss: 0.25525525212287903\n",
            "Epoch: 111 Loss: 0.24669954180717468\n",
            "Epoch: 112 Loss: 0.23849505186080933\n",
            "Epoch: 113 Loss: 0.23062995076179504\n",
            "Epoch: 114 Loss: 0.22309164702892303\n",
            "Epoch: 115 Loss: 0.2158670872449875\n",
            "Epoch: 116 Loss: 0.20894284546375275\n",
            "Epoch: 117 Loss: 0.20230555534362793\n",
            "Epoch: 118 Loss: 0.1959422528743744\n",
            "Epoch: 119 Loss: 0.1898403763771057\n",
            "Epoch: 120 Loss: 0.18398787081241608\n",
            "Epoch: 121 Loss: 0.17837348580360413\n",
            "Epoch: 122 Loss: 0.17298641800880432\n",
            "Epoch: 123 Loss: 0.16781672835350037\n",
            "Epoch: 124 Loss: 0.16285492479801178\n",
            "Epoch: 125 Loss: 0.1580919623374939\n",
            "Epoch: 126 Loss: 0.15351952612400055\n",
            "Epoch: 127 Loss: 0.14912980794906616\n",
            "Epoch: 128 Loss: 0.1449154168367386\n",
            "Epoch: 129 Loss: 0.14086958765983582\n",
            "Epoch: 130 Loss: 0.13698585331439972\n",
            "Epoch: 131 Loss: 0.1332579404115677\n",
            "Epoch: 132 Loss: 0.12967970967292786\n",
            "Epoch: 133 Loss: 0.12624509632587433\n",
            "Epoch: 134 Loss: 0.12294820696115494\n",
            "Epoch: 135 Loss: 0.11978313326835632\n",
            "Epoch: 136 Loss: 0.1167442575097084\n",
            "Epoch: 137 Loss: 0.11382601410150528\n",
            "Epoch: 138 Loss: 0.11102314293384552\n",
            "Epoch: 139 Loss: 0.1083303913474083\n",
            "Epoch: 140 Loss: 0.10574265569448471\n",
            "Epoch: 141 Loss: 0.1032550260424614\n",
            "Epoch: 142 Loss: 0.10086283087730408\n",
            "Epoch: 143 Loss: 0.09856162965297699\n",
            "Epoch: 144 Loss: 0.09634730964899063\n",
            "Epoch: 145 Loss: 0.094215989112854\n",
            "Epoch: 146 Loss: 0.09216409176588058\n",
            "Epoch: 147 Loss: 0.09018824249505997\n",
            "Epoch: 148 Loss: 0.08828514814376831\n",
            "Epoch: 149 Loss: 0.08645173907279968\n",
            "Epoch: 150 Loss: 0.0846850574016571\n",
            "Epoch: 151 Loss: 0.08298216760158539\n",
            "Epoch: 152 Loss: 0.0813402608036995\n",
            "Epoch: 153 Loss: 0.0797567218542099\n",
            "Epoch: 154 Loss: 0.07822898030281067\n",
            "Epoch: 155 Loss: 0.07675457745790482\n",
            "Epoch: 156 Loss: 0.07533116638660431\n",
            "Epoch: 157 Loss: 0.07395651191473007\n",
            "Epoch: 158 Loss: 0.07262851297855377\n",
            "Epoch: 159 Loss: 0.07134506106376648\n",
            "Epoch: 160 Loss: 0.07010432332754135\n",
            "Epoch: 161 Loss: 0.0689043402671814\n",
            "Epoch: 162 Loss: 0.0677434504032135\n",
            "Epoch: 163 Loss: 0.06661989539861679\n",
            "Epoch: 164 Loss: 0.06553219258785248\n",
            "Epoch: 165 Loss: 0.06447877734899521\n",
            "Epoch: 166 Loss: 0.06345823407173157\n",
            "Epoch: 167 Loss: 0.06246917322278023\n",
            "Epoch: 168 Loss: 0.061510298401117325\n",
            "Epoch: 169 Loss: 0.060580383986234665\n",
            "Epoch: 170 Loss: 0.059678226709365845\n",
            "Epoch: 171 Loss: 0.05880273878574371\n",
            "Epoch: 172 Loss: 0.057952817529439926\n",
            "Epoch: 173 Loss: 0.057127438485622406\n",
            "Epoch: 174 Loss: 0.05632562190294266\n",
            "Epoch: 175 Loss: 0.05554645135998726\n",
            "Epoch: 176 Loss: 0.05478905513882637\n",
            "Epoch: 177 Loss: 0.05405253544449806\n",
            "Epoch: 178 Loss: 0.05333614721894264\n",
            "Epoch: 179 Loss: 0.05263908952474594\n",
            "Epoch: 180 Loss: 0.051960695534944534\n",
            "Epoch: 181 Loss: 0.05130021274089813\n",
            "Epoch: 182 Loss: 0.050657011568546295\n",
            "Epoch: 183 Loss: 0.050030454993247986\n",
            "Epoch: 184 Loss: 0.04941994324326515\n",
            "Epoch: 185 Loss: 0.048824917525053024\n",
            "Epoch: 186 Loss: 0.04824480414390564\n",
            "Epoch: 187 Loss: 0.04767909273505211\n",
            "Epoch: 188 Loss: 0.04712728410959244\n",
            "Epoch: 189 Loss: 0.04658889397978783\n",
            "Epoch: 190 Loss: 0.04606343433260918\n",
            "Epoch: 191 Loss: 0.04555053263902664\n",
            "Epoch: 192 Loss: 0.04504970461130142\n",
            "Epoch: 193 Loss: 0.04456061124801636\n",
            "Epoch: 194 Loss: 0.04408281669020653\n",
            "Epoch: 195 Loss: 0.04361594840884209\n",
            "Epoch: 196 Loss: 0.04315970093011856\n",
            "Epoch: 197 Loss: 0.04271369427442551\n",
            "Epoch: 198 Loss: 0.04227760434150696\n",
            "Epoch: 199 Loss: 0.04185113683342934\n",
            "Epoch: 200 Loss: 0.041433993726968765\n",
            "Epoch: 201 Loss: 0.04102586582303047\n",
            "Epoch: 202 Loss: 0.040626462548971176\n",
            "Epoch: 203 Loss: 0.04023556411266327\n",
            "Epoch: 204 Loss: 0.03985288739204407\n",
            "Epoch: 205 Loss: 0.03947816789150238\n",
            "Epoch: 206 Loss: 0.0391111858189106\n",
            "Epoch: 207 Loss: 0.03875169903039932\n",
            "Epoch: 208 Loss: 0.038399506360292435\n",
            "Epoch: 209 Loss: 0.03805439919233322\n",
            "Epoch: 210 Loss: 0.03771615028381348\n",
            "Epoch: 211 Loss: 0.037384577095508575\n",
            "Epoch: 212 Loss: 0.037059489637613297\n",
            "Epoch: 213 Loss: 0.036740709096193314\n",
            "Epoch: 214 Loss: 0.03642803430557251\n",
            "Epoch: 215 Loss: 0.03612131252884865\n",
            "Epoch: 216 Loss: 0.03582041710615158\n",
            "Epoch: 217 Loss: 0.035525113344192505\n",
            "Epoch: 218 Loss: 0.03523531183600426\n",
            "Epoch: 219 Loss: 0.03495084121823311\n",
            "Epoch: 220 Loss: 0.034671563655138016\n",
            "Epoch: 221 Loss: 0.034397341310977936\n",
            "Epoch: 222 Loss: 0.03412804380059242\n",
            "Epoch: 223 Loss: 0.033863555639982224\n",
            "Epoch: 224 Loss: 0.03360369801521301\n",
            "Epoch: 225 Loss: 0.03334842249751091\n",
            "Epoch: 226 Loss: 0.03309757262468338\n",
            "Epoch: 227 Loss: 0.032851044088602066\n",
            "Epoch: 228 Loss: 0.03260873630642891\n",
            "Epoch: 229 Loss: 0.03237054869532585\n",
            "Epoch: 230 Loss: 0.032136380672454834\n",
            "Epoch: 231 Loss: 0.0319061204791069\n",
            "Epoch: 232 Loss: 0.03167968988418579\n",
            "Epoch: 233 Loss: 0.03145698830485344\n",
            "Epoch: 234 Loss: 0.031237920746207237\n",
            "Epoch: 235 Loss: 0.03102242574095726\n",
            "Epoch: 236 Loss: 0.030810387805104256\n",
            "Epoch: 237 Loss: 0.030601762235164642\n",
            "Epoch: 238 Loss: 0.03039645217359066\n",
            "Epoch: 239 Loss: 0.030194388702511787\n",
            "Epoch: 240 Loss: 0.029995473101735115\n",
            "Epoch: 241 Loss: 0.029799683019518852\n",
            "Epoch: 242 Loss: 0.029606902971863747\n",
            "Epoch: 243 Loss: 0.029417116194963455\n",
            "Epoch: 244 Loss: 0.029230201616883278\n",
            "Epoch: 245 Loss: 0.029046138748526573\n",
            "Epoch: 246 Loss: 0.02886486053466797\n",
            "Epoch: 247 Loss: 0.028686268255114555\n",
            "Epoch: 248 Loss: 0.028510363772511482\n",
            "Epoch: 249 Loss: 0.028337066993117332\n",
            "Epoch: 250 Loss: 0.028166325762867928\n",
            "Epoch: 251 Loss: 0.027998073026537895\n",
            "Epoch: 252 Loss: 0.02783227525651455\n",
            "Epoch: 253 Loss: 0.02766888216137886\n",
            "Epoch: 254 Loss: 0.027507830411195755\n",
            "Epoch: 255 Loss: 0.027349069714546204\n",
            "Epoch: 256 Loss: 0.02719259262084961\n",
            "Epoch: 257 Loss: 0.02703833021223545\n",
            "Epoch: 258 Loss: 0.026886222884058952\n",
            "Epoch: 259 Loss: 0.02673625573515892\n",
            "Epoch: 260 Loss: 0.02658836916089058\n",
            "Epoch: 261 Loss: 0.026442524045705795\n",
            "Epoch: 262 Loss: 0.02629869617521763\n",
            "Epoch: 263 Loss: 0.026156846433877945\n",
            "Epoch: 264 Loss: 0.02601691707968712\n",
            "Epoch: 265 Loss: 0.025878876447677612\n",
            "Epoch: 266 Loss: 0.025742707774043083\n",
            "Epoch: 267 Loss: 0.025608351454138756\n",
            "Epoch: 268 Loss: 0.02547580562531948\n",
            "Epoch: 269 Loss: 0.02534501627087593\n",
            "Epoch: 270 Loss: 0.025215933099389076\n",
            "Epoch: 271 Loss: 0.025088561698794365\n",
            "Epoch: 272 Loss: 0.02496284991502762\n",
            "Epoch: 273 Loss: 0.024838771671056747\n",
            "Epoch: 274 Loss: 0.024716300889849663\n",
            "Epoch: 275 Loss: 0.024595392867922783\n",
            "Epoch: 276 Loss: 0.02447604201734066\n",
            "Epoch: 277 Loss: 0.024358220398426056\n",
            "Epoch: 278 Loss: 0.02424187771975994\n",
            "Epoch: 279 Loss: 0.02412700466811657\n",
            "Epoch: 280 Loss: 0.024013569578528404\n",
            "Epoch: 281 Loss: 0.0239015631377697\n",
            "Epoch: 282 Loss: 0.023790957406163216\n",
            "Epoch: 283 Loss: 0.02368169091641903\n",
            "Epoch: 284 Loss: 0.02357378415763378\n",
            "Epoch: 285 Loss: 0.02346719615161419\n",
            "Epoch: 286 Loss: 0.023361893370747566\n",
            "Epoch: 287 Loss: 0.02325788140296936\n",
            "Epoch: 288 Loss: 0.023155130445957184\n",
            "Epoch: 289 Loss: 0.023053616285324097\n",
            "Epoch: 290 Loss: 0.022953301668167114\n",
            "Epoch: 291 Loss: 0.02285417914390564\n",
            "Epoch: 292 Loss: 0.02275625243782997\n",
            "Epoch: 293 Loss: 0.02265946939587593\n",
            "Epoch: 294 Loss: 0.022563813254237175\n",
            "Epoch: 295 Loss: 0.02246929332613945\n",
            "Epoch: 296 Loss: 0.022375857457518578\n",
            "Epoch: 297 Loss: 0.02228350006043911\n",
            "Epoch: 298 Loss: 0.02219220995903015\n",
            "Epoch: 299 Loss: 0.022101975977420807\n",
            "Epoch: 300 Loss: 0.022012757137417793\n",
            "Epoch: 301 Loss: 0.021924549713730812\n",
            "Epoch: 302 Loss: 0.02183735929429531\n",
            "Epoch: 303 Loss: 0.021751124411821365\n",
            "Epoch: 304 Loss: 0.021665863692760468\n",
            "Epoch: 305 Loss: 0.021581564098596573\n",
            "Epoch: 306 Loss: 0.021498216316103935\n",
            "Epoch: 307 Loss: 0.021415766328573227\n",
            "Epoch: 308 Loss: 0.021334223449230194\n",
            "Epoch: 309 Loss: 0.021253570914268494\n",
            "Epoch: 310 Loss: 0.021173812448978424\n",
            "Epoch: 311 Loss: 0.02109491266310215\n",
            "Epoch: 312 Loss: 0.021016860380768776\n",
            "Epoch: 313 Loss: 0.02093966118991375\n",
            "Epoch: 314 Loss: 0.020863289013504982\n",
            "Epoch: 315 Loss: 0.020787712186574936\n",
            "Epoch: 316 Loss: 0.020712966099381447\n",
            "Epoch: 317 Loss: 0.020639002323150635\n",
            "Epoch: 318 Loss: 0.020565805956721306\n",
            "Epoch: 319 Loss: 0.020493391901254654\n",
            "Epoch: 320 Loss: 0.02042173594236374\n",
            "Epoch: 321 Loss: 0.020350804552435875\n",
            "Epoch: 322 Loss: 0.020280631259083748\n",
            "Epoch: 323 Loss: 0.020211176946759224\n",
            "Epoch: 324 Loss: 0.020142419263720512\n",
            "Epoch: 325 Loss: 0.02007439360022545\n",
            "Epoch: 326 Loss: 0.020007042214274406\n",
            "Epoch: 327 Loss: 0.019940389320254326\n",
            "Epoch: 328 Loss: 0.01987440139055252\n",
            "Epoch: 329 Loss: 0.019809063524007797\n",
            "Epoch: 330 Loss: 0.019744401797652245\n",
            "Epoch: 331 Loss: 0.019680388271808624\n",
            "Epoch: 332 Loss: 0.01961700990796089\n",
            "Epoch: 333 Loss: 0.01955425553023815\n",
            "Epoch: 334 Loss: 0.019492125138640404\n",
            "Epoch: 335 Loss: 0.01943059451878071\n",
            "Epoch: 336 Loss: 0.019369691610336304\n",
            "Epoch: 337 Loss: 0.019309356808662415\n",
            "Epoch: 338 Loss: 0.019249631091952324\n",
            "Epoch: 339 Loss: 0.019190451130270958\n",
            "Epoch: 340 Loss: 0.019131897017359734\n",
            "Epoch: 341 Loss: 0.01907387375831604\n",
            "Epoch: 342 Loss: 0.019016390666365623\n",
            "Epoch: 343 Loss: 0.01895947940647602\n",
            "Epoch: 344 Loss: 0.018903082236647606\n",
            "Epoch: 345 Loss: 0.018847275525331497\n",
            "Epoch: 346 Loss: 0.01879194937646389\n",
            "Epoch: 347 Loss: 0.01873714290559292\n",
            "Epoch: 348 Loss: 0.018682878464460373\n",
            "Epoch: 349 Loss: 0.01862909086048603\n",
            "Epoch: 350 Loss: 0.01857583597302437\n",
            "Epoch: 351 Loss: 0.01852303184568882\n",
            "Epoch: 352 Loss: 0.018470752984285355\n",
            "Epoch: 353 Loss: 0.0184189360588789\n",
            "Epoch: 354 Loss: 0.018367616459727287\n",
            "Epoch: 355 Loss: 0.01831674762070179\n",
            "Epoch: 356 Loss: 0.01826634258031845\n",
            "Epoch: 357 Loss: 0.018216412514448166\n",
            "Epoch: 358 Loss: 0.018166907131671906\n",
            "Epoch: 359 Loss: 0.018117856234312057\n",
            "Epoch: 360 Loss: 0.018069270998239517\n",
            "Epoch: 361 Loss: 0.01802108623087406\n",
            "Epoch: 362 Loss: 0.017973333597183228\n",
            "Epoch: 363 Loss: 0.01792602427303791\n",
            "Epoch: 364 Loss: 0.01787913776934147\n",
            "Epoch: 365 Loss: 0.01783263124525547\n",
            "Epoch: 366 Loss: 0.017786581069231033\n",
            "Epoch: 367 Loss: 0.017740894109010696\n",
            "Epoch: 368 Loss: 0.017695613205432892\n",
            "Epoch: 369 Loss: 0.017650751397013664\n",
            "Epoch: 370 Loss: 0.017606263980269432\n",
            "Epoch: 371 Loss: 0.017562154680490494\n",
            "Epoch: 372 Loss: 0.017518430948257446\n",
            "Epoch: 373 Loss: 0.017475085332989693\n",
            "Epoch: 374 Loss: 0.01743212155997753\n",
            "Epoch: 375 Loss: 0.017389491200447083\n",
            "Epoch: 376 Loss: 0.017347244545817375\n",
            "Epoch: 377 Loss: 0.017305362969636917\n",
            "Epoch: 378 Loss: 0.017263835296034813\n",
            "Epoch: 379 Loss: 0.017222637310624123\n",
            "Epoch: 380 Loss: 0.01718181185424328\n",
            "Epoch: 381 Loss: 0.01714129000902176\n",
            "Epoch: 382 Loss: 0.017101138830184937\n",
            "Epoch: 383 Loss: 0.01706131175160408\n",
            "Epoch: 384 Loss: 0.017021814361214638\n",
            "Epoch: 385 Loss: 0.016982633620500565\n",
            "Epoch: 386 Loss: 0.0169437974691391\n",
            "Epoch: 387 Loss: 0.016905255615711212\n",
            "Epoch: 388 Loss: 0.016867052763700485\n",
            "Epoch: 389 Loss: 0.01682915911078453\n",
            "Epoch: 390 Loss: 0.016791556030511856\n",
            "Epoch: 391 Loss: 0.01675426959991455\n",
            "Epoch: 392 Loss: 0.01671728678047657\n",
            "Epoch: 393 Loss: 0.016680588945746422\n",
            "Epoch: 394 Loss: 0.016644198447465897\n",
            "Epoch: 395 Loss: 0.016608091071248055\n",
            "Epoch: 396 Loss: 0.01657227799296379\n",
            "Epoch: 397 Loss: 0.016536742448806763\n",
            "Epoch: 398 Loss: 0.01650148816406727\n",
            "Epoch: 399 Loss: 0.016466520726680756\n",
            "Epoch: 400 Loss: 0.01643182709813118\n",
            "Epoch: 401 Loss: 0.016397399827837944\n",
            "Epoch: 402 Loss: 0.016363251954317093\n",
            "Epoch: 403 Loss: 0.01632935367524624\n",
            "Epoch: 404 Loss: 0.01629573665559292\n",
            "Epoch: 405 Loss: 0.016262371093034744\n",
            "Epoch: 406 Loss: 0.016229262575507164\n",
            "Epoch: 407 Loss: 0.01619642972946167\n",
            "Epoch: 408 Loss: 0.016163820400834084\n",
            "Epoch: 409 Loss: 0.01613147184252739\n",
            "Epoch: 410 Loss: 0.01609937846660614\n",
            "Epoch: 411 Loss: 0.01606753095984459\n",
            "Epoch: 412 Loss: 0.016035905107855797\n",
            "Epoch: 413 Loss: 0.016004540026187897\n",
            "Epoch: 414 Loss: 0.015973398461937904\n",
            "Epoch: 415 Loss: 0.015942495316267014\n",
            "Epoch: 416 Loss: 0.015911849215626717\n",
            "Epoch: 417 Loss: 0.01588139683008194\n",
            "Epoch: 418 Loss: 0.01585119217634201\n",
            "Epoch: 419 Loss: 0.015821190550923347\n",
            "Epoch: 420 Loss: 0.015791431069374084\n",
            "Epoch: 421 Loss: 0.015761882066726685\n",
            "Epoch: 422 Loss: 0.01573256216943264\n",
            "Epoch: 423 Loss: 0.015703445300459862\n",
            "Epoch: 424 Loss: 0.01567455567419529\n",
            "Epoch: 425 Loss: 0.015645870938897133\n",
            "Epoch: 426 Loss: 0.01561739481985569\n",
            "Epoch: 427 Loss: 0.015589122660458088\n",
            "Epoch: 428 Loss: 0.015561068430542946\n",
            "Epoch: 429 Loss: 0.015533221885561943\n",
            "Epoch: 430 Loss: 0.015505541115999222\n",
            "Epoch: 431 Loss: 0.015478085726499557\n",
            "Epoch: 432 Loss: 0.015450824052095413\n",
            "Epoch: 433 Loss: 0.015423755161464214\n",
            "Epoch: 434 Loss: 0.015396878123283386\n",
            "Epoch: 435 Loss: 0.015370212495326996\n",
            "Epoch: 436 Loss: 0.015343712642788887\n",
            "Epoch: 437 Loss: 0.015317395329475403\n",
            "Epoch: 438 Loss: 0.015291286632418633\n",
            "Epoch: 439 Loss: 0.015265347436070442\n",
            "Epoch: 440 Loss: 0.015239585191011429\n",
            "Epoch: 441 Loss: 0.015214021317660809\n",
            "Epoch: 442 Loss: 0.015188626013696194\n",
            "Epoch: 443 Loss: 0.015163405798375607\n",
            "Epoch: 444 Loss: 0.01513836719095707\n",
            "Epoch: 445 Loss: 0.015113488771021366\n",
            "Epoch: 446 Loss: 0.015088796615600586\n",
            "Epoch: 447 Loss: 0.015064281411468983\n",
            "Epoch: 448 Loss: 0.015039918944239616\n",
            "Epoch: 449 Loss: 0.015015721321105957\n",
            "Epoch: 450 Loss: 0.01499168947339058\n",
            "Epoch: 451 Loss: 0.014967828057706356\n",
            "Epoch: 452 Loss: 0.014944142661988735\n",
            "Epoch: 453 Loss: 0.014920592308044434\n",
            "Epoch: 454 Loss: 0.014897217974066734\n",
            "Epoch: 455 Loss: 0.014874001033604145\n",
            "Epoch: 456 Loss: 0.014850922860205173\n",
            "Epoch: 457 Loss: 0.01482801791280508\n",
            "Epoch: 458 Loss: 0.014805263839662075\n",
            "Epoch: 459 Loss: 0.014782646670937538\n",
            "Epoch: 460 Loss: 0.014760198071599007\n",
            "Epoch: 461 Loss: 0.01473789568990469\n",
            "Epoch: 462 Loss: 0.014715735800564289\n",
            "Epoch: 463 Loss: 0.014693708159029484\n",
            "Epoch: 464 Loss: 0.014671855606138706\n",
            "Epoch: 465 Loss: 0.014650137163698673\n",
            "Epoch: 466 Loss: 0.014628555625677109\n",
            "Epoch: 467 Loss: 0.014607115648686886\n",
            "Epoch: 468 Loss: 0.014585799537599087\n",
            "Epoch: 469 Loss: 0.01456465944647789\n",
            "Epoch: 470 Loss: 0.014543617144227028\n",
            "Epoch: 471 Loss: 0.014522740617394447\n",
            "Epoch: 472 Loss: 0.014501997269690037\n",
            "Epoch: 473 Loss: 0.014481370337307453\n",
            "Epoch: 474 Loss: 0.014460894279181957\n",
            "Epoch: 475 Loss: 0.014440534636378288\n",
            "Epoch: 476 Loss: 0.01442030631005764\n",
            "Epoch: 477 Loss: 0.014400227926671505\n",
            "Epoch: 478 Loss: 0.014380245469510555\n",
            "Epoch: 479 Loss: 0.014360412023961544\n",
            "Epoch: 480 Loss: 0.01434068102389574\n",
            "Epoch: 481 Loss: 0.014321103692054749\n",
            "Epoch: 482 Loss: 0.014301641844213009\n",
            "Epoch: 483 Loss: 0.014282286167144775\n",
            "Epoch: 484 Loss: 0.014263073913753033\n",
            "Epoch: 485 Loss: 0.014243965037167072\n",
            "Epoch: 486 Loss: 0.014224975369870663\n",
            "Epoch: 487 Loss: 0.014206109568476677\n",
            "Epoch: 488 Loss: 0.014187362045049667\n",
            "Epoch: 489 Loss: 0.01416872814297676\n",
            "Epoch: 490 Loss: 0.014150213450193405\n",
            "Epoch: 491 Loss: 0.0141318179666996\n",
            "Epoch: 492 Loss: 0.014113512821495533\n",
            "Epoch: 493 Loss: 0.014095338992774487\n",
            "Epoch: 494 Loss: 0.014077252708375454\n",
            "Epoch: 495 Loss: 0.014059322886168957\n",
            "Epoch: 496 Loss: 0.014041462913155556\n",
            "Epoch: 497 Loss: 0.014023734256625175\n",
            "Epoch: 498 Loss: 0.014006094075739384\n",
            "Epoch: 499 Loss: 0.013988574035465717\n",
            "Epoch: 500 Loss: 0.013971147127449512\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# پیش‌بینی کلمه بعدی در یک جمله ساده\n",
        "input_text = 'alireza nansouri work as1'\n",
        "predicted_text = predict(model, input_text, vocab, tokenizer)\n",
        "a=predicted_text.replace('  ',',')\n",
        "a=a.replace(' ','')\n",
        "a=a.replace(',',' ')\n",
        "#print(a)\n",
        "from collections import Counter\n",
        "\n",
        "sentences = a.split('.')\n",
        "\n",
        "\n",
        "# شمارش تعداد تکرار هر جمله\n",
        "sentence_counts = Counter(sentences)\n",
        "# جملاتی که بیش از یک بار تکرار شده‌اند\n",
        "repeated_sentences = [sentence for sentence in sentences if sentence_counts[sentence] > 1]\n",
        "\n",
        "# جملات بدون تکرار\n",
        "unique_sentences = list(set(sentences))\n",
        "unique_sentences=[i for i in unique_sentences if input_text in i]\n",
        "print(max(unique_sentences,key=len))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWRbRPFEt_fM",
        "outputId": "607a0ce2-e737-43c1-c22c-e0cb376c955e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "balireza nansouri work as1 ai enginieering a\n"
          ]
        }
      ]
    }
  ]
}